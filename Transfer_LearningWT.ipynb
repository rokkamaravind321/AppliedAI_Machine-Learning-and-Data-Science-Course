{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0zs3TflcAEr"
   },
   "source": [
    "<pre>\n",
    "1. Download all the data in this folder https://drive.google.com/open?id=1Z4TyI7FcFVEx8qdl4jO9qxvxaqLSqoEu. it contains two file both images and labels. The label file list the images and their categories in the following format:\n",
    "            <b>path/to/the/image.tif,category</b>\n",
    "            \n",
    "    where the categories are numbered 0 to 15, in the following order:\n",
    "\n",
    "    <b>0 letter\n",
    "    1 form\n",
    "    2 email\n",
    "    3 handwritten\n",
    "    4 advertisement\n",
    "    5 scientific report\n",
    "    6 scientific publication\n",
    "    7 specification\n",
    "    8 file folder\n",
    "    9 news article\n",
    "    10 budget\n",
    "    11 invoice\n",
    "    12 presentation\n",
    "    13 questionnaire\n",
    "    14 resume\n",
    "    15 memo</b>\n",
    "    \n",
    "2. On this image data, you have to train 3 types of models as given below. You have to split the data into Train and Validation data.\n",
    "\n",
    "3. Try not to load all the images into memory, use the gernarators that we have given the reference notebooks to load the batch of images only during the train data.\n",
    "or you can use this method also\n",
    "<a href='https://medium.com/@vijayabhaskar96/tutorial-on-keras-imagedatagenerator-with-flow-from-dataframe-8bd5776e45c1'>https://medium.com/@vijayabhaskar96/tutorial-on-keras-imagedatagenerator-with-flow-from-dataframe-8bd5776e45c1</a>\n",
    "\n",
    "<a href='https://medium.com/@vijayabhaskar96/tutorial-on-keras-flow-from-dataframe-1fd4493d237c'>https://medium.com/@vijayabhaskar96/tutorial-on-keras-flow-from-dataframe-1fd4493d237c</a>\n",
    "\n",
    "\n",
    "4. You are free to choose Learning rate, optimizer, loss function, image augmentation, any hyperparameters. but you have to use the same architechture what we are asking below. \n",
    "\n",
    "5. Use tensorboard for every model and analyse your gradients. (you need to upload the screenshots for each model for evaluation)\n",
    "\n",
    "Note: fit_genarator() method will have problems with the tensorboard histograms, try to debug it, if you could not do use histgrams=0 i.e don't include histograms, check the documentation of tensorboard for more information. \n",
    "\n",
    "6. You can check about Transfer Learning in this link - <a href='https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html'>https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html</a>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZXpEZtJcAEu"
   },
   "source": [
    "### Model-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "De0UlsaOcAE1"
   },
   "source": [
    "### Model-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amKbfojfcAE-"
   },
   "source": [
    "### Model-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EsSl-PcDtEhv"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QOWb0OFStNaW"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense,Input,Conv2D,MaxPool2D,Activation,Dropout,Flatten,GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "import random as rn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7zXUWhVtPoc",
    "outputId": "1cc915a4-54f6-49b4-9a85-c1bd9bbd03c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Z4TyI7FcFVEx8qdl4jO9qxvxaqLSqoEu\n",
      "To: /content/rvl-cdip.rar\n",
      "4.66GB [01:25, 54.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown --id 1Z4TyI7FcFVEx8qdl4jO9qxvxaqLSqoEu\n",
    "\n",
    "\n",
    "get_ipython().system_raw(\"unrar x rvl-cdip.rar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4uduTvRLtOFg",
    "outputId": "877dec72-b4ee-432f-c7f2-601728fc7d24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.config', 'data_final', 'rvl-cdip.rar', 'labels_final.csv', 'sample_data']"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JxSBiakW8D6O"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('labels_final.csv') #reading the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "9wHMY8lO8Ghv",
    "outputId": "8d9c833d-11cd-4935-ed0e-ed4f97e3dac9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>imagesv/v/o/h/voh71d00/509132755+-2755.tif</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>imagesl/l/x/t/lxt19d00/502213303.tif</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>imagesx/x/e/d/xed05a00/2075325674.tif</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>imageso/o/j/b/ojb60d00/517511301+-1301.tif</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>imagesq/q/z/k/qzk17e00/2031320195.tif</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         path  label\n",
       "0  imagesv/v/o/h/voh71d00/509132755+-2755.tif      3\n",
       "1        imagesl/l/x/t/lxt19d00/502213303.tif      3\n",
       "2       imagesx/x/e/d/xed05a00/2075325674.tif      2\n",
       "3  imageso/o/j/b/ojb60d00/517511301+-1301.tif      3\n",
       "4       imagesq/q/z/k/qzk17e00/2031320195.tif      7"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "-oe0-nwm97zm"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#traindf, validationdf = train_test_split(data, test_size=0.3, random_state=42,shuffle=True)\n",
    "\n",
    "train_path, validation_path, train_label, validation_label = train_test_split(data['path'], data['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "rqSM1T9k8Kbl"
   },
   "outputs": [],
   "source": [
    "labels_dict = { 0 : 'letter',1: 'form',2: 'email',3 :'handwritten',4 :'advertisement',\n",
    "                5 : 'scientific report',6 : 'scientific publication',7 :'specification',8 : 'file folder',\n",
    "                9 : 'news article', 10 : 'budget', 11 : 'invoice',12 : 'presentation',\n",
    "               13 : 'questionnaire', 14 : 'resume', 15 : 'memo'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89um3Wx1VhIq",
    "outputId": "56c910f3-9413-4890-8ec2-c35ba6169e6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values(['letter', 'form', 'email', 'handwritten', 'advertisement', 'scientific report', 'scientific publication', 'specification', 'file folder', 'news article', 'budget', 'invoice', 'presentation', 'questionnaire', 'resume', 'memo'])"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mXG31DU1AOaq"
   },
   "outputs": [],
   "source": [
    "for subfolder_name in list(labels_dict.values()):\n",
    "    os.makedirs(os.path.join('train_images', subfolder_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KDLTiZLxMiE9",
    "outputId": "8ae91f83-7b33-453c-d1b0-d33242d9826d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['advertisement',\n",
       " 'handwritten',\n",
       " 'memo',\n",
       " 'form',\n",
       " 'invoice',\n",
       " 'email',\n",
       " 'file folder',\n",
       " 'scientific publication',\n",
       " 'presentation',\n",
       " 'budget',\n",
       " 'news article',\n",
       " 'questionnaire',\n",
       " 'letter',\n",
       " 'resume',\n",
       " 'scientific report',\n",
       " 'specification']"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('train_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bG-StjG-MlvY",
    "outputId": "aa54f1b9-8ce6-49f4-d9e9-2749a72bb653"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38400it [02:08, 299.08it/s]\n"
     ]
    }
   ],
   "source": [
    "#https://thispointer.com/python-how-to-copy-files-from-one-location-to-another-using-shutil-copy/\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "for file, label in tqdm(zip(train_path, train_label)):\n",
    "  shutil.copy('data_final/'+file,'train_images/'+labels_dict[label]+'/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YZbpkzxlcDJn"
   },
   "outputs": [],
   "source": [
    "for subfolder_name in list(labels_dict.values()):\n",
    "    os.makedirs(os.path.join('validation_images', subfolder_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KXZNc1RocaHp",
    "outputId": "b593010c-a25d-4162-baf9-248ff196e067"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9600it [00:34, 279.55it/s]\n"
     ]
    }
   ],
   "source": [
    "for file, label in tqdm(zip(validation_path, validation_label)):\n",
    "    shutil.copy('data_final/'+file ,'validation_images/'+labels_dict[label]+'/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2o-SSha2ZTL1",
    "outputId": "ee6595e0-6307-4f6b-e47a-9498e18a04f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Images in  advertisement  category is  2393\n",
      "No of Images in  handwritten  category is  2413\n",
      "No of Images in  memo  category is  2401\n",
      "No of Images in  form  category is  2407\n",
      "No of Images in  invoice  category is  2397\n",
      "No of Images in  email  category is  2379\n",
      "No of Images in  file folder  category is  2351\n",
      "No of Images in  scientific publication  category is  2373\n",
      "No of Images in  presentation  category is  2428\n",
      "No of Images in  budget  category is  2422\n",
      "No of Images in  news article  category is  2392\n",
      "No of Images in  questionnaire  category is  2395\n",
      "No of Images in  letter  category is  2461\n",
      "No of Images in  resume  category is  2415\n",
      "No of Images in  scientific report  category is  2379\n",
      "No of Images in  specification  category is  2391\n"
     ]
    }
   ],
   "source": [
    "dir_path= 'train_images'\n",
    "for i in os.listdir(dir_path):\n",
    "    print(\"No of Images in \",i,\" category is \",len(os.listdir(os.path.join(dir_path,i))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1KeI9kalb7BX",
    "outputId": "457de46c-341b-4ed5-b2c7-a09554c6f5a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Images in  advertisement  category is  601\n",
      "No of Images in  handwritten  category is  592\n",
      "No of Images in  memo  category is  595\n",
      "No of Images in  form  category is  587\n",
      "No of Images in  invoice  category is  595\n",
      "No of Images in  email  category is  614\n",
      "No of Images in  file folder  category is  652\n",
      "No of Images in  scientific publication  category is  612\n",
      "No of Images in  presentation  category is  578\n",
      "No of Images in  budget  category is  580\n",
      "No of Images in  news article  category is  609\n",
      "No of Images in  questionnaire  category is  612\n",
      "No of Images in  letter  category is  554\n",
      "No of Images in  resume  category is  590\n",
      "No of Images in  scientific report  category is  620\n",
      "No of Images in  specification  category is  609\n"
     ]
    }
   ],
   "source": [
    "dir_path= 'validation_images'\n",
    "for i in os.listdir(dir_path):\n",
    "    print(\"No of Images in \",i,\" category is \",len(os.listdir(os.path.join(dir_path,i))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ow7rZoHkdovA",
    "outputId": "ea10bc84-c8b4-4643-ac34-4b58a47536d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 38397 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "dir_path= 'train_images'\n",
    "ImageFlow = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "ImageGenerator_train = ImageFlow.flow_from_directory(dir_path,target_size=(224,224),seed=10,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iBWs22iMglfg",
    "outputId": "0f81f169-c19f-4bd6-e1c7-6db37bc75984"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9600 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "dir_path='validation_images'\n",
    "ImageGenerator_validation = ImageFlow.flow_from_directory(dir_path,target_size=(224,224),seed=10,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4QkArLK69ebq",
    "outputId": "15246922-795d-4d95-cce5-039a62c3c806"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216 batches: 34.13801717758179 s\n",
      "202.47222 Images/s\n"
     ]
    }
   ],
   "source": [
    "##Checking time taken to load images. \n",
    "import time\n",
    "start = time.time()\n",
    "total_batches = 0\n",
    "\n",
    "batches = 0\n",
    "per_batch = 32\n",
    "for x_batch, y_batch in ImageGenerator_train:\n",
    "    batches += 1\n",
    "    if batches >= 6899/per_batch:\n",
    "        total_batches = total_batches + batches\n",
    "        break \n",
    "end = time.time()\n",
    "duration = end-start\n",
    "print(\"{} batches: {} s\".format(total_batches, duration))\n",
    "print(\"{:0.5f} Images/s\".format(per_batch*total_batches/duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EF12MYu1cAEy"
   },
   "source": [
    "<pre>\n",
    "1. Use <a href='https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG16'>VGG-16</a> pretrained network without Fully Connected layers and initilize all the weights with Imagenet trained weights. \n",
    "2. After VGG-16 network without FC layers, add a new Conv block ( 1 Conv layer and 1 Maxpooling ), 2 FC layers and a output layer to classify 16 classes. You are free to choose any hyperparameters/parameters of conv block, FC layers, output layer. \n",
    "3. Final architecture will be <b>INPUT --> VGG-16 without Top layers(FC) --> Conv Layer --> Maxpool Layer --> 2 FC layers --> Output Layer</b>\n",
    "4. Train only new Conv block, FC layers, output layer. Don't train the VGG-16 network. \n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DlI2FH259efQ",
    "outputId": "b403764f-907a-4d34-f9c5-f90a8c591707"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "##https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "## Have to clear the session. If you are not clearing, Graph will create again and again and graph size will increses. \n",
    "## Varibles will also set to some value from before session\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "## Set the random seed values to regenerate the model.\n",
    "np.random.seed(0)\n",
    "rn.seed(0)\n",
    "\n",
    "\n",
    "#VGG-16\n",
    "base_model=tf.keras.applications.VGG16(weights='imagenet', include_top=False,input_shape=(224,224,3))\n",
    "\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x=base_model.output\n",
    "\n",
    "#print(x)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "     layer.trainable = False\n",
    "\n",
    "#base_model.trainable=False\n",
    "\n",
    "#Conv Layer\n",
    "Conv1 = Conv2D(filters=32,kernel_size=(3,3),strides=(1,1),padding='valid',data_format='channels_last',\n",
    "              activation='relu',kernel_initializer=tf.keras.initializers.he_normal(seed=0),name='Conv1')(x)\n",
    "#MaxPool Layer\n",
    "Pool1 = MaxPool2D(pool_size=(2,2),strides=(2,2),padding='valid',data_format='channels_last',name='Pool1')(Conv1)\n",
    "\n",
    "flatten = Flatten(data_format='channels_last',name='Flatten')(Pool1)\n",
    "\n",
    "#FC layer\n",
    "FC1 = Dense(units=64,activation='relu',kernel_initializer=tf.keras.initializers.glorot_normal(seed=32),name='FC1')(flatten)\n",
    "\n",
    "#FC layer\n",
    "FC2 = Dense(units=32,activation='relu',kernel_initializer=tf.keras.initializers.glorot_normal(seed=33),name='FC2')(FC1)\n",
    "\n",
    "#output layer\n",
    "Out = Dense(units=16,activation='softmax',kernel_initializer=tf.keras.initializers.glorot_normal(seed=3),name='Output')(FC2)\n",
    "\n",
    "#Creating a model\n",
    "model_1 = Model(inputs=base_model.input,outputs=Out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nXaPUSBhtOIz",
    "outputId": "335a687a-647b-45d3-a35a-6628dcd6d408"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv2D)               (None, 5, 5, 32)          147488    \n",
      "_________________________________________________________________\n",
      "Pool1 (MaxPooling2D)         (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "Flatten (Flatten)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "FC2 (Dense)                  (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 16)                528       \n",
      "=================================================================\n",
      "Total params: 14,873,040\n",
      "Trainable params: 158,352\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "SVBCSzkAtONM"
   },
   "outputs": [],
   "source": [
    "model_1.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=[\"accuracy\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ztOElzQrY2k-",
    "outputId": "ca4f8342-7717-40d3-a077-a242e61e1a5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1200/1200 [==============================] - 277s 203ms/step - loss: 2.2030 - accuracy: 0.3689 - val_loss: 1.5048 - val_accuracy: 0.5334\n",
      "Epoch 2/3\n",
      "1200/1200 [==============================] - 222s 185ms/step - loss: 1.4179 - accuracy: 0.5628 - val_loss: 1.4221 - val_accuracy: 0.5724\n",
      "Epoch 3/3\n",
      "1200/1200 [==============================] - 188s 157ms/step - loss: 1.2840 - accuracy: 0.6057 - val_loss: 1.4315 - val_accuracy: 0.5847\n"
     ]
    }
   ],
   "source": [
    "history=model_1.fit(ImageGenerator_train,\n",
    "                    validation_data=ImageGenerator_validation,epochs=3,steps_per_epoch=len(ImageGenerator_train),validation_steps=len(ImageGenerator_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "98lHrGtKD9m6",
    "outputId": "8fe66742-e21e-49ff-e98f-395654d3843c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.45391565561294556, 0.5649399757385254, 0.6053076982498169],\n",
       " 'loss': [1.7981104850769043, 1.4176541566848755, 1.3013049364089966],\n",
       " 'val_accuracy': [0.5334374904632568, 0.5723958611488342, 0.5846874713897705],\n",
       " 'val_loss': [1.5048243999481201, 1.422074317932129, 1.4315128326416016]}"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "M7SntA_vXHog"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xZLXpd6mW7kH",
    "outputId": "3c44b241-87ba-4384-d3ae-fdd6506fcd4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 validated image filenames belonging to 0 classes.\n",
      "Found 0 validated image filenames belonging to 0 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/dataframe_iterator.py:282: UserWarning: Found 33600 invalid image filename(s) in x_col=\"path\". These filename(s) will be ignored.\n",
      "  .format(n_invalid, x_col)\n",
      "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/dataframe_iterator.py:282: UserWarning: Found 33600 invalid image filename(s) in x_col=\"path\". These filename(s) will be ignored.\n",
      "  .format(n_invalid, x_col)\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('./labels_final.csv',dtype=str)\n",
    "traindf, validationdf = train_test_split(df, test_size=0.3, random_state=42,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hx93Ky5gvbk"
   },
   "outputs": [],
   "source": [
    "datagen=ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "train_generator=datagen.flow_from_dataframe(dataframe=traindf,directory='./train_images/',x_col=\"path\",\n",
    "y_col=\"label\",batch_size=32,seed=42,shuffle=False,class_mode=\"categorical\",target_size=(224,224,3),subset='training')\n",
    "\n",
    "valid_generator=datagen.flow_from_dataframe(dataframe=traindf,directory='./validation_images/',x_col=\"path\",\n",
    "y_col=\"label\",batch_size=32,seed=42,shuffle=False,class_mode=\"categorical\",target_size=(224,224,3),subset='validation')\n",
    "  \n",
    "# Define your model\n",
    "\n",
    "# model.compile(optimizer='adam', loss=......, metrics=[\"accuracy\"])\n",
    "\n",
    "# model.fit(train_generator,validation_data=validation_generator,epochs=10,steps_per_epoch=len(train_generator),validation_steps=len(validation_generator))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNXN3EXFcAE5"
   },
   "source": [
    "<pre>\n",
    "1. Use <a href='https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG16'>VGG-16</a> pretrained network without Fully Connected layers and initilize all the weights with Imagenet trained weights.\n",
    "2. After VGG-16 network without FC layers, don't use FC layers, use conv layers only as Fully connected layer. any FC layer can be converted to a CONV layer. This conversion will reduce the No of Trainable parameters in FC layers. For example, an FC layer with K=4096 that is looking at some input volume of size 7×7×512 can be equivalently expressed as a CONV layer with F=7,P=0,S=1,K=4096. In other words, we are setting the filter size to be exactly the size of the input volume, and hence the output will simply be 1×1×4096 since only a single depth column “fits” across the input volume, giving identical result as the initial FC layer. You can refer <a href='http://cs231n.github.io/convolutional-networks/#convert'>this</a> link to better understanding of using Conv layer in place of fully connected layers.\n",
    "3. Final architecture will be VGG-16 without FC layers(without top), 2 Conv layers identical to FC layers, 1 output layer for 16 class classification. <b>INPUT --> VGG-16 without Top layers(FC) --> 2 Conv Layers identical to FC --> Output Layer</b>\n",
    "3. Train only last 2 Conv layers identical to FC layers, 1 output layer. Don't train the VGG-16 network. \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CI1t59J-ML5i",
    "outputId": "890dbdb0-1ed6-4bae-f497-35f1699640af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "## Set the random seed values to regenerate the model.\n",
    "np.random.seed(0)\n",
    "rn.seed(0)\n",
    "\n",
    "\n",
    "#VGG-16\n",
    "base_model_2=tf.keras.applications.VGG16(weights='imagenet', include_top=False,input_shape=(224,224,3))\n",
    "print(base_model_2.input)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x=base_model_2.output\n",
    "\n",
    "for layer in base_model_2.layers:\n",
    "     layer.trainable = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Conv1 = Conv2D(4096,kernel_size=[7,7],strides=(1,1),padding='valid', activation='relu',name='Conv1')(x)\n",
    "\n",
    "Conv2 = Conv2D(4096,kernel_size=[1,1],strides=(1,1),padding='valid',data_format='channels_last', \n",
    "               activation='relu',kernel_initializer=tf.keras.initializers.he_normal(seed=0),name='Conv2')(Conv1)\n",
    "\n",
    "flatten = Flatten(data_format='channels_last',name='Flatten')(Conv2)\n",
    "output = Dense(units=16,activation='softmax',kernel_initializer=tf.keras.initializers.glorot_normal(seed=3),name='Output')(flatten)\n",
    "\n",
    "# #Creating a model\n",
    "model_2 = Model(inputs=base_model_2.input,outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CHehdEA-MMWY",
    "outputId": "bac0bf86-d568-4512-da25-b89cd28697bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv2D)               (None, 1, 1, 4096)        102764544 \n",
      "_________________________________________________________________\n",
      "Conv2 (Conv2D)               (None, 1, 1, 4096)        16781312  \n",
      "_________________________________________________________________\n",
      "Flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 16)                65552     \n",
      "=================================================================\n",
      "Total params: 134,326,096\n",
      "Trainable params: 119,611,408\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bLqRPhsOYZcb",
    "outputId": "beb0931d-6f73-4ca3-9c55-4830b0d8bc38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1200/1200 [==============================] - 209s 173ms/step - loss: 21.8210 - accuracy: 0.3893 - val_loss: 1.4671 - val_accuracy: 0.6195\n",
      "Epoch 2/3\n",
      "1200/1200 [==============================] - 207s 173ms/step - loss: 1.2502 - accuracy: 0.6540 - val_loss: 1.2839 - val_accuracy: 0.6566\n",
      "Epoch 3/3\n",
      "1200/1200 [==============================] - 207s 173ms/step - loss: 1.0990 - accuracy: 0.7062 - val_loss: 1.2574 - val_accuracy: 0.6760\n"
     ]
    }
   ],
   "source": [
    "model_2.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history_2=model_2.fit(ImageGenerator_train,\n",
    "                    validation_data=ImageGenerator_validation,epochs=3,steps_per_epoch=len(ImageGenerator_train),validation_steps=len(ImageGenerator_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9AULF-PcAFC"
   },
   "source": [
    "<pre>\n",
    "1. Use same network as Model-2 '<b>INPUT --> VGG-16 without Top layers(FC) --> 2 Conv Layers identical to FC --> Output Layer</b>' and train only Last 6 Layers of VGG-16 network, 2 Conv layers identical to FC layers, 1 output layer.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geTnLyPyoBbx"
   },
   "source": [
    "MODEL 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "R44aWp4-MMdH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "## Set the random seed values to regenerate the model.\n",
    "np.random.seed(0)\n",
    "rn.seed(0)\n",
    "\n",
    "\n",
    "#VGG-16\n",
    "base_model_3=tf.keras.applications.VGG16(weights='imagenet', include_top=False,input_shape=(224,224,3))\n",
    "\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x=base_model_3.output\n",
    "\n",
    "\n",
    "\n",
    "for layer in base_model_3.layers[:13]:\n",
    "  layer.trainable = False\n",
    "\n",
    "\n",
    "# # #Conv Layer\n",
    "Conv1 = Conv2D(4096,kernel_size=[7,7],strides=(1,1),padding='valid', activation='relu',name='Conv1')(x)\n",
    "\n",
    "Conv2 = Conv2D(4096,kernel_size=[1,1],strides=(1,1),padding='valid',data_format='channels_last', \n",
    "               activation='relu',kernel_initializer=tf.keras.initializers.he_normal(seed=0),name='Conv2')(Conv1)\n",
    "flatten = Flatten(data_format='channels_last',name='Flatten')(Conv2)\n",
    "output = Dense(units=16,activation='softmax',kernel_initializer=tf.keras.initializers.glorot_normal(seed=3),name='Output')(flatten)\n",
    "\n",
    "# #Creating a model\n",
    "model_3 = Model(inputs=base_model_3.input,outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BTYwP2fDMMe_",
    "outputId": "24440361-cc5b-41d4-f9cb-8241bd356371"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "Conv1 (Conv2D)               (None, 1, 1, 4096)        102764544 \n",
      "_________________________________________________________________\n",
      "Conv2 (Conv2D)               (None, 1, 1, 4096)        16781312  \n",
      "_________________________________________________________________\n",
      "Flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 16)                65552     \n",
      "=================================================================\n",
      "Total params: 134,326,096\n",
      "Trainable params: 129,050,640\n",
      "Non-trainable params: 5,275,456\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GDhKWHLRMMsW",
    "outputId": "4560cd34-0bce-4f5d-bed3-9d2635ced578"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1200/1200 [==============================] - 284s 236ms/step - loss: 227.2454 - accuracy: 0.0603 - val_loss: 2.7729 - val_accuracy: 0.0604\n",
      "Epoch 2/3\n",
      "1200/1200 [==============================] - 284s 236ms/step - loss: 2.7727 - accuracy: 0.0593 - val_loss: 2.7731 - val_accuracy: 0.0577\n",
      "Epoch 3/3\n",
      "1200/1200 [==============================] - 284s 236ms/step - loss: 2.7728 - accuracy: 0.0641 - val_loss: 2.7731 - val_accuracy: 0.0577\n"
     ]
    }
   ],
   "source": [
    "model_3.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history_3=model_3.fit(ImageGenerator_train,\n",
    "                    validation_data=ImageGenerator_validation,epochs=3,steps_per_epoch=len(ImageGenerator_train),validation_steps=len(ImageGenerator_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UtSS1wycTr8D",
    "outputId": "d94a76c2-6ca4-4372-f9ef-0a57216755f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.06013490632176399, 0.0614110492169857, 0.06370289623737335],\n",
       " 'loss': [39.831398010253906, 2.772803544998169, 2.7727930545806885],\n",
       " 'val_accuracy': [0.06041666492819786,\n",
       "  0.05770833417773247,\n",
       "  0.05770833417773247],\n",
       " 'val_loss': [2.772860527038574, 2.773062229156494, 2.7731451988220215]}"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_3.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAJV4UCMX_WZ"
   },
   "source": [
    "MODEL 1 Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PkjynquPMM2r",
    "outputId": "cf3df027-c0f0-457d-e211-80e6d6b9d73b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.53076982498169"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['accuracy'][2]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GRR-zaS9YBmL"
   },
   "source": [
    "MODEL 2 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oN0lbtRLMNCh",
    "outputId": "3133fdba-c7c7-4f43-8d86-0544ccd29ea0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.18256783485413"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_2.history['accuracy'][2]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0T1dO4HYCOb"
   },
   "source": [
    "MODEL 3 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eHtKlbVhXzdG",
    "outputId": "e7ba7ffd-8282-4686-8922-619555a2b6c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.370289623737335"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_3.history['accuracy'][2]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Last model Accuracy was low because we tried to train the weight of the vgg model with our own data and thus it didnt perform well."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Transfer Learning.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
